<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>浙江大学超算队</title>
  
  <subtitle>Zhejiang University Supercomputer Team</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://zjusct.github.io/"/>
  <updated>2019-05-26T08:02:50.177Z</updated>
  <id>https://zjusct.github.io/</id>
  
  <author>
    <name>ZJU · SCT</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于0-1乘性噪声的朴素图片降噪</title>
    <link href="https://zjusct.github.io/2019/05/11/Image-Restoration-SimpleVersion/"/>
    <id>https://zjusct.github.io/2019/05/11/Image-Restoration-SimpleVersion/</id>
    <published>2019-05-10T16:00:00.000Z</published>
    <updated>2019-05-26T08:02:50.177Z</updated>
    
    <content type="html"><![CDATA[<h2 id="项目内容"><a href="#项目内容" class="headerlink" title="项目内容"></a>项目内容</h2><p>给定3张受损图像，尝试恢复他们的原始图像。</p><ol><li>原始图像包含1张黑白图像（A.png）和2张彩色图像（B.png, C.png）。</li><li>受损图像$X$是由原始图像$I \in R ^ { H * W * C }$添加了不同噪声遮罩$M \in R ^ { H * W * C }$得到的$x=I \odot m$，其中$ \odot $是逐元素相乘。</li><li>噪声遮罩仅包含{0,1}值。对应原图（A/B/C）的噪声遮罩的每行分别用0.8/0.4/0.6的噪声比率产生的，即噪声遮罩每个通道每行80%/40%/60%的像素值为0，其他为1。</li></ol><p><strong>评估误差为恢复图像与原始图像向量化后的差向量的2-范数，此误差越小越好。</strong></p><a id="more"></a><br><h2 id="实现介绍"><a href="#实现介绍" class="headerlink" title="实现介绍"></a>实现介绍</h2><br><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>由于图片的像素点在空间上满足局部相似的特征，相邻的像素点通道值变化往往是平滑且有一定规则的。因此，我们可以用一个模型来拟合像素点通道值在空间上的关系。具体实现中，我们将图片切割成若干个小矩形块，然后使用一个二维线性回归模型来回归每个小矩形块中位置和像素通道值的函数关系。为了使结果更加平滑和可靠，我们采用了高斯函数作为基函数。</p><br><h3 id="高斯函数"><a href="#高斯函数" class="headerlink" title="高斯函数"></a>高斯函数</h3><p>当我们对一个 ss * ss 的像素矩阵块做回归的时候，我们要把所有没有被噪音损坏的点都提取出来。点位置用一个高斯核函数处理，这样原来的点坐标(x,y)就被转换成了$(e^{-\frac{(x-mid)^2}{2}},e^{-\frac{(y-mid)^2}{2}})$，在特征空间中用来刻画这个点与矩阵块中心的距离。这样，我们实际要回归的就是点心距与像素通道值的关系。</p><p>这样做，主要是因为我们的局部性原理本身就是不带方向性的，所谓的局部性就是指临近的点存在某种平滑的变化关系。使用这样一个衡量距离的核函数，可以使得我们的回归结果更加平滑：</p><p><img src="1.png" alt="Non-Gaussian"><img src="2.png" alt="Gaussian"></p><p>上图左边是用坐标直接回归，右图是坐标经过高斯核处理后回归的结果。可以看到左边有大量的不平滑的交错的黑白点，看起来很“脏”。右边由于采用了高斯函数处理过的表征距离关系的核函数，结果更加平滑，清晰。</p><p>CODE : </p><p><img src="0.png" alt="CODE OF GAUSSIAN KERNEL"></p><br><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>提取完特征后的数据点拟合，就是一个简单的线性回归任务而已，我们采用最小二乘法回归。<br>$$<br>Loss = \frac {\Sigma_{i=0}^n(y_i-\phi(x_i) * w^T)^2}{n}=\frac{\Sigma_{i=1}^{n}Loss_i}{n}<br>$$<br>使用随机梯度下降来最优化损失函数：<br>$$<br>w\leftarrow w-\eta * \triangledown Loss_i=w+2\eta(y_i-\phi(x_i) * w^T) * \phi(x_i)<br>$$<br>具体实现中，我们取步长=0.005，进行100轮随机梯度下降。</p><p>CODE : </p><p><img src="3.png" alt="CODE OF TRAINING"></p><p>​    <br></p><h3 id="迭代降噪"><a href="#迭代降噪" class="headerlink" title="迭代降噪"></a>迭代降噪</h3><p>我们前面提到了，我们要把图片切成若干个小矩形块，对每个小矩形块分别进行回归，那么这个小矩形块的尺寸取多少比较合适呢？</p><p>我们先取ss=2尝试一下：</p><p><img src="4.png" alt="SS=2"></p><p>噪音为0.8的时候，我们可以看到，如果取一个2*2的块，期望其中没损坏的通道只有0.8个，所以势必有大量的矩阵块里面都是全损坏的，这会使得一些全损坏的块得不到修复，产生大量的黑点。</p><p>直接取ss=5：</p><p><img src="5.png" alt="SS=5"></p><p>显然，黑块的数量变少了，但是实际上图片给人的颗粒赶很明显，更像是一堆模糊的马赛克拼图拼凑而成的。</p><p>我们的解决办法是先取ss=2，对图片做恢复，然后将恢复的图像再用更大的ss来恢复。</p><p>下面是用ss={2，3，4，5}迭代恢复四次的过程：</p><p><img src="6.png" alt="SS-&gt;2"><img src="7.png" alt="SS-&gt;2-&gt;3">    </p><p><img src="8.png" alt="SS-&gt;2-&gt;3-&gt;4"><img src="9.png" alt="SS-&gt;2-&gt;3-&gt;4-&gt;5"></p><p>可以看到黑点逐渐被消除，并且最后经过ss=5的回归后得到的结果在视觉效果上明显优于直接用ss=5进行回归。这种想法本质上是一种<font color="red">贪心算法</font>。先将损失密度小的局部块恢复好，再充分利用之前修复出来的信息将损失密度更大的块修复。</p><br><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>A （noise rate = 0.8）</p><p><img src="10.png" alt="NR=0.8_原图"><img src="9.png" alt="NR=0.8_修复图"></p><p>B（noise rate = 0.4）</p><p><img src="11.png" alt="NR=0.4_原图"><img src="12.png" alt="NR=0.4_修复图"></p><p>C（noise rate = 0.6）</p><p><img src="13.png" alt="NR=0.6_原图"><img src="14.png" alt="NR=0.6_修复图"></p><p>D（根据原图自己生成，测试迭代过程中损失的减少）</p><p><img src="15.png" alt="误差测试"></p><br><h2 id="潜在的优化展望"><a href="#潜在的优化展望" class="headerlink" title="潜在的优化展望"></a>潜在的优化展望</h2><ol><li>由于每个块的修复是独立的，可以考虑使用CPU多线程计算或者在GPU上用CUDA进行并行优化，加速整个修复过程。</li><li>使用更复杂的网络来拟合。</li><li>使用马尔科夫随机场的方法来做图像降噪。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;项目内容&quot;&gt;&lt;a href=&quot;#项目内容&quot; class=&quot;headerlink&quot; title=&quot;项目内容&quot;&gt;&lt;/a&gt;项目内容&lt;/h2&gt;&lt;p&gt;给定3张受损图像，尝试恢复他们的原始图像。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;原始图像包含1张黑白图像（A.png）和2张彩色图像（B.png, C.png）。&lt;/li&gt;
&lt;li&gt;受损图像$X$是由原始图像$I \in R ^ { H * W * C }$添加了不同噪声遮罩$M \in R ^ { H * W * C }$得到的$x=I \odot m$，其中$ \odot $是逐元素相乘。&lt;/li&gt;
&lt;li&gt;噪声遮罩仅包含{0,1}值。对应原图（A/B/C）的噪声遮罩的每行分别用0.8/0.4/0.6的噪声比率产生的，即噪声遮罩每个通道每行80%/40%/60%的像素值为0，其他为1。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;评估误差为恢复图像与原始图像向量化后的差向量的2-范数，此误差越小越好。&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://zjusct.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Virtual Memory and TLB</title>
    <link href="https://zjusct.github.io/2019/03/31/Virtual-Memory-and-TLB/"/>
    <id>https://zjusct.github.io/2019/03/31/Virtual-Memory-and-TLB/</id>
    <published>2019-03-31T06:05:11.000Z</published>
    <updated>2019-05-26T08:02:50.189Z</updated>
    
    <content type="html"><![CDATA[<h2 id="虚拟地址空间"><a href="#虚拟地址空间" class="headerlink" title="虚拟地址空间"></a>虚拟地址空间</h2><p>x86 CPU 的地址总选宽度为32位，理论寻址上限为4GB。而虚拟地址空间的大小就是4GB，占满总线，且空间中的每一个字节分配一个虚拟地址</p><ul><li>其中高2G<code>0x80000000 ~ 0xFFFFFFFF</code>为内和空间，由操作系统调用；</li><li>低2G<code>0x00000000 ~ 0x7FFFFFFF</code>为用户空间，由用户使用。</li></ul><p>在系统中运行的每一个进程都独自拥有一个虚拟空间，进城之间的虚拟空间不共用。</p><a id="more"></a><p>虚拟地址空间是一种通过机制映射出来的空间，与实际物理空间大小无必然联系，在x86保护模式下，无论计算及实际主存是512MB还是8GB，虚拟地址空间总是4GB，<strong>这是由CPU和操作系统的宽度决定的</strong>，即：</p><blockquote><p>CPU地址总线宽度 → 物理地址范围<br>CPU的ALU宽度 → 操作系统位数 → 虚拟地址范围</p></blockquote><h3 id="虚拟内存"><a href="#虚拟内存" class="headerlink" title="虚拟内存"></a>虚拟内存</h3><p>虚拟地址空间 = 主存 + 虚拟内存(交换空间 Swap Space)</p><p>虚拟内存：将硬盘的一部分作为存储器使用，来扩充物理内存。</p><p>利用了自动覆盖、交换技术。内存中存不下、暂时不用的内容会存在硬盘中。</p><blockquote><p>Assume: 32位操作系统，32位寻址总线宽度 → 4G线性空间</p></blockquote><h2 id="保护模式下的进程运行"><a href="#保护模式下的进程运行" class="headerlink" title="保护模式下的进程运行"></a>保护模式下的进程运行</h2><p>虚拟地址空间是硬件行为，CPU自动完成(同时与操作系统协作)虚拟地址到物理地址(可能差熬过实际内存，这样会产生一个异常中断，揭晓来有操作系统处理(如从虚拟内存中调出对应的页框内容))。</p><p>所以，一个程序若运行在保护模式下，其汇编级、机器语言级的寻址都是用的虚拟地址，即在一般的编程中不会接触到物理一层。</p><p>在进程被加载时，系统为进程建立唯一的数据结构<code>进程控制块(PCB = Process Control Block)</code>，直至进程结束。</p><p>PCB中描述了该进程的现状以及控制运行的全部信息，有了PCB，一个进程才可以在保护模式下和其他进程一起被并发地运行起来，操作系统通过PCB对进程进行控制。</p><p>PCB中的程序ID(PID(unix、linux)、句柄(windows))是进程的唯一标识；PCB中的一个指针指向 <strong>页表</strong> ，这些都与地址转化有关。</p><h2 id="地址转化"><a href="#地址转化" class="headerlink" title="地址转化"></a>地址转化</h2><p>地址转化的全过程可以用以下这张图来概括：</p><p><img src="OG.png" alt="OG"></p><p>以下是具体步骤介绍。</p><h3 id="1-逻辑地址-→-线性地址-段式内存管理，Intel早期策略的保留"><a href="#1-逻辑地址-→-线性地址-段式内存管理，Intel早期策略的保留" class="headerlink" title="1. 逻辑地址 → 线性地址 (段式内存管理，Intel早期策略的保留)"></a>1. 逻辑地址 → 线性地址 (段式内存管理，Intel早期策略的保留)</h3><ul><li><p>段内偏移地址(32位)</p></li><li><p>段选择符：16位长的序列，是索引值，定位段描述符；结构：<br><img src="%232.png" alt="#2"></p><ul><li>高13位为表内索引号 —— 但注意由于GDT第一项留空，所以索引要先加1；</li><li>而2位为TI表指示器，0是指GDT，1是指LDT；</li><li>0、1位是RPL请求者特权级，00最高，11最低 —— 在x86保护模式下修改寄存器是系统之灵，必须有对应的权限才能修改(当前执行权限和段寄存器中(被修改的)的RPL均不低于目标段的RPL)</li></ul></li><li><p>段描述符：8x8=64位长的结构，用来描述一个段的各种属性。结构：<br> <img src="%231.png" alt="#1"></p><ul><li>0、1字节+6字节低4位(20位) 段边界/段长度：最大1MB或者4G(看粒度位的单位)</li><li>2、3、4、7字节(32位) 段基址：4G线性地址的任意位置(不一定非要被16整除)</li><li>6、7字节的奇怪设计是为了兼容80286(24位地址总线)</li><li>剩下的那些是段属性，详见<code>20180819143434</code></li></ul></li><li><p>段描述表：多任务操作系统中，含有多个任务，而每个人物都有多个段，其段描述符存于段描述表中。<br>IA-32处理器有3个段描述表：GDT、LDT和IDT。</p><ul><li>GDT(Global Descripter Table) 全局段描述符表：一个系统一般只有一个GDT，含有每一个任务都可以访问的段；通常包含操作系统所使用的代码段、数据段和堆栈段，GDT同时包含各进程LDT数据段项，以及进程间通讯所需要的段。<br>GDTR是CPU提供的寄存器，存储GDT的位置和边界；在32位模式下RGDT有48位长(高32位基地址+低16位边界)，在32e模式下有80位长(高64位基地址+低16位边界)。<br>GDT的第一个表项留空不用，是空描述符，所以索引号要加1。<br>GDT最多128项。</li><li>LDT(Local Descripter Table) 局部段描述符表：16位长，属于某个进程。一个进程一个LDT，对应有RLDT寄存器，进程切换时RLDT改变。<br>RLDT和RGDT不一样，RLDT是一个索引值而不是实际指向，指向GDT中某一个LDT描述项。所以如果要获取LDT中的某一项，先要访问GDT找到对应LDT，再找到LDT中的一项。<br>编译程序时，程序内赋予了虚拟页号。在程序运行时，通过对应LDT转译成物理地址。故虚拟页号是局部性的、不同进程的页号会有冲突。<br>LDT没有空选择子。</li><li>IDT(Interrupt Descripter Table) 中断段描述符表；一个系统一般也只有一个。</li><li>以下这个图能做一点解释：<br><img src="%237.png" alt="#7"></li></ul></li></ul><h3 id="2-线性地址-→-物理地址-页式内存管理"><a href="#2-线性地址-→-物理地址-页式内存管理" class="headerlink" title="2. 线性地址 → 物理地址 (页式内存管理)"></a>2. 线性地址 → 物理地址 (页式内存管理)</h3><p>这一步由CPU的页式管理单元来负责转换。——MMU(内存管理单元)。</p><ul><li><p>线性地址可以拆分为三部分(或者两部分)：<br><img src="%233.png" alt="#3"></p></li><li><p>页(Page)：线性地址被划分为大小一致的若干内存区域，其对应映射到大小相同的与物理空间区域页框(Frame)上。这个映射不一定是连贯而有序的。</p></li><li><p>CR3：页目录基址寄存器。对于每一个进程，CR3的内容不同(有点像RLDT)，页目录基址也不同，线性地址-物理地址的映射也不同。</p></li><li><p>页目录：占用一个4kb的内存页，最多存储1024个页目录表项(PDE)，一个PDE有4字节。在没启用PAE时，有两种PDE，规格不同。</p></li><li><p>页目录表项(PDE)：每个程序有多个页表，即拥有多个PDE。PDE的结构如下：<br><img src="%234.png" alt="#4"><br>12~31位(20位)表示页表起始物理地址的高20位(页表基址低12位为0，即一定以4kb对齐)。</p></li><li><p>页表：一个页表占4kb的内存页，最多存储1024个页表项(PTE)，一个PTE是4字节。页表的基址是4kb对齐的，低12位是0。</p></li></ul><p>采用对页表项的二级管理模式(也目录→页表→页)能够节约空间。因为不存在的页表就可以不分配空间，并且对于Windows来说只有一级页表才会存在主存中，二级可以存在辅存中——不过Linux中它们都常驻主存。</p><p>一些CPU会提供更多级的架构，如三级、四级。Linux中，有对应的高层次抽象，提供了一个四层页管理架构：<br><img src="%236.png" alt="#6"><br>把中间的某几个定为长度为0，就可以调整架构级数。如“四化二”：某地址0x08147258，对应的PUD、PMD里只有一个表项为PUD→PMD，PMD→PT；划分的时候，PGD=0000100000，PUD=PMD=0，PT=0101000111.</p><h3 id="3-TLB-转换检测缓冲区、快表、转译后被缓冲区"><a href="#3-TLB-转换检测缓冲区、快表、转译后被缓冲区" class="headerlink" title="3. TLB (转换检测缓冲区、快表、转译后被缓冲区)"></a>3. TLB (转换检测缓冲区、快表、转译后被缓冲区)</h3><p>处理器中，一个具有并行朝赵能力的特殊高速缓存器，存储最近访问过的一些页表项(时空局部性原理，减少页映射的内存访问次数)。</p><p>TLB较贵，通常能够存放16~512个页表项。</p><ul><li><p>TLB命中：直接取出对应的页表项</p></li><li><p>TLB缺失：先淘汰TLB中的某一项(TLB替换策略，一些算法，可以由硬件或软件来实现)</p><ul><li>硬件处理TLB Miss：CPU会遍历页表，找到正确的PTE；如果没有找到，CPU就会发起一个页错误并将控制权交给操作系统。</li><li>软件处理TLB Miss：CPU直接发出未命中错误，让操作系统来处理。</li></ul></li><li><p>脏记录：当TLB中某个PTE项失效(如切换进程、进程退出、虚拟页换出到磁盘)，PTE标记为不存在，此时映射已经不成立了。<br>操作系统要保证即时刷新掉这些脏记录，不同的CPU有不同的刷新TLB方法，但每次都完全刷新TLB会很慢，所以现在有一些策略，扩展对一个PTE的描述(如针对某个进程、空间的标识，如果目前进程与PTE相关，就会忽略掉)，这样可以让多个进程同时共存TLB</p></li></ul><h2 id="Linux-段式管理"><a href="#Linux-段式管理" class="headerlink" title="Linux 段式管理"></a>Linux 段式管理</h2><p>Linux似乎没有理会Intel的那一套段的机制，而是做了一个高级的抽象。<br>Linux对所有的进程使用了相同的段来对指令和数据寻址，让每个段寄存器都指向同一个段描述符，让这个段描述符的基址为0，长度为4G。即用这种方式略去了段式内存管理。<br>对应多有用户代码段、用户数据段、内核代码段和内核数据段。可以在<code>segment.h</code>中看到，四种段对应的段基址都是0，这就是“平坦内存模型”，这样就有<code>段内偏移地址=逻辑地址</code></p><p>且，四种段对应的都为GDT。即Linux大多数情况都不使用LDT，除非使用wine等Windows防真程序。</p><p>Linux 0.11中每个进程划分64MB的虚拟内存空间。故逻辑地址范围为0~0x4000000</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;虚拟地址空间&quot;&gt;&lt;a href=&quot;#虚拟地址空间&quot; class=&quot;headerlink&quot; title=&quot;虚拟地址空间&quot;&gt;&lt;/a&gt;虚拟地址空间&lt;/h2&gt;&lt;p&gt;x86 CPU 的地址总选宽度为32位，理论寻址上限为4GB。而虚拟地址空间的大小就是4GB，占满总线，且空间中的每一个字节分配一个虚拟地址&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中高2G&lt;code&gt;0x80000000 ~ 0xFFFFFFFF&lt;/code&gt;为内和空间，由操作系统调用；&lt;/li&gt;
&lt;li&gt;低2G&lt;code&gt;0x00000000 ~ 0x7FFFFFFF&lt;/code&gt;为用户空间，由用户使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在系统中运行的每一个进程都独自拥有一个虚拟空间，进城之间的虚拟空间不共用。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Operating Susyem" scheme="https://zjusct.github.io/tags/Operating-Susyem/"/>
    
  </entry>
  
  <entry>
    <title>如何在墙内快速部署CentOS 7的MySQL</title>
    <link href="https://zjusct.github.io/2019/03/31/Install-MySQL-on-CentOS7-Inside-GFW/"/>
    <id>https://zjusct.github.io/2019/03/31/Install-MySQL-on-CentOS7-Inside-GFW/</id>
    <published>2019-03-31T02:45:19.000Z</published>
    <updated>2019-05-26T08:02:50.189Z</updated>
    
    <content type="html"><![CDATA[<p>MySQL 被 Oracle 收购后，CentOS 的镜像仓库中提供的默认的数据库也变为了 MariaDB，所以默认没有 MySQL ，需要手动安装。</p><p>其实安装 MySQL 也并不是一件很难的事情，但是由于一些实际存在的问题(比如某墙)，让默认通过 yum 安装 MySQL 的速度太慢。这里提出一种可行的方案来快速部署 MySQL ，此方案同样适用于其他 rpm 包软件的手动安装。</p><p>本文实际在讲的是，如何利用各种手段，加速和改善yum的安装过程。</p><a id="more"></a><h2 id="传统方案……慢到怀疑人生"><a href="#传统方案……慢到怀疑人生" class="headerlink" title="传统方案……慢到怀疑人生"></a>传统方案……慢到怀疑人生</h2><p>根据<a href="https://dev.mysql.com/downloads/repo/yum/" target="_blank" rel="noopener">官方指南</a>，我们执行如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载源</span></span><br><span class="line">wget <span class="string">"https://dev.mysql.com/get/mysql80-community-release-el7-2.noarch.rpm"</span></span><br><span class="line"><span class="comment"># 安装源</span></span><br><span class="line">sudo rpm -ivh mysql80-community-release-el7-2.noarch.rpm</span><br><span class="line"><span class="comment"># 检查源是否成功安装</span></span><br><span class="line">sudo yum repolist enabled | grep <span class="string">"mysql80-community*"</span></span><br></pre></td></tr></table></figure><p>接下来就是正常的安装步骤：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install mysql-community-server mysql</span><br></pre></td></tr></table></figure><p>但是由于一些原因，下载速度基本是几Byte/s，MySQL 服务器的大小(加上依赖服务)差不多有600MB，这种方法基本不可取。手头没有特别好的而且很新的软件源，就打算手动安装。</p><h2 id="手动安装法"><a href="#手动安装法" class="headerlink" title="手动安装法"></a>手动安装法</h2><p>首先依然需要下载并安装官方源。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install mysql-community-server</span><br></pre></td></tr></table></figure><p>利用该命令我们可以获取一些 MySQl Server 以来安装顺序及其版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">=================================================================================================</span><br><span class="line"> Package                       Arch          Version              Repository                Size</span><br><span class="line">=================================================================================================</span><br><span class="line">Reinstalling:</span><br><span class="line"> mysql-community-client        x86_64        8.0.15-1.el7         mysql80-community         25 M</span><br><span class="line"></span><br><span class="line"> mysql-community-libs          x86_64        8.0.15-1.el7         mysql80-community          2 M</span><br><span class="line"></span><br><span class="line"> mysql-community-common        x86_64        8.0.15-1.el7         mysql80-community        570 K</span><br><span class="line"></span><br><span class="line"> mysql-community-server        x86_64        8.0.15-1.el7         mysql80-community        360 M</span><br><span class="line"></span><br><span class="line">Transaction Summary</span><br><span class="line">=================================================================================================</span><br></pre></td></tr></table></figure><p>解压并分析rpm源包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm2cpio mysql80-community-release-el7-2.noarch.rpm | cpio -div</span><br><span class="line">vim /etc/yum.repos.d/mysql-community.repo</span><br></pre></td></tr></table></figure><p>从中我们可以找到对应版本的网络路径为<code>http://repo.mysql.com/yum/mysql-8.0-community/el/7/x86_64/</code>。</p><p>打开该地址，找到对应的几个安装包：</p><ul><li>mysql-community-client-8.0.15-1.el7.x86_64.rpm</li><li>mysql-community-libs-8.0.15-1.el7.x86_64.rpm</li><li>mysql-community-common-8.0.15-1.el7.x86_64.rpm</li><li>mysql-community-server-8.0.15-1.el7.x86_64.rpm</li></ul><p>使用某种下载工具(我使用的是迅雷)下载，然后使用<code>scp</code>指令上传到服务器上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scp mysql-community-client-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-client-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">scp mysql-community-libs-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-libs-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">scp mysql-community-common-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-common-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">scp mysql-community-server-8.0.15-1.el7.x86_64.rpm xxx@xx.xx.xx.xx:/root/mysql-community-server-8.0.15-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><p>按照先后顺序依次执行<code>yum</code>本地安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo yum localinstall mysql-community-common-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">sudo yum localinstall mysql-community-libs-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">sudo yum localinstall mysql-community-client-8.0.15-1.el7.x86_64.rpm</span><br><span class="line">sudo yum localinstall mysql-community-server-8.0.15-1.el7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">sudo yum -y install mysql</span><br></pre></td></tr></table></figure><p>安装成功，启动并测试服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mysqld.service</span><br><span class="line">systemctl status mysqld.service</span><br></pre></td></tr></table></figure><p>找出默认密码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep &quot;password&quot; /var/log/mysqld.log &gt;&gt; defalut_mysql_passwd.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MySQL 被 Oracle 收购后，CentOS 的镜像仓库中提供的默认的数据库也变为了 MariaDB，所以默认没有 MySQL ，需要手动安装。&lt;/p&gt;
&lt;p&gt;其实安装 MySQL 也并不是一件很难的事情，但是由于一些实际存在的问题(比如某墙)，让默认通过 yum 安装 MySQL 的速度太慢。这里提出一种可行的方案来快速部署 MySQL ，此方案同样适用于其他 rpm 包软件的手动安装。&lt;/p&gt;
&lt;p&gt;本文实际在讲的是，如何利用各种手段，加速和改善yum的安装过程。&lt;/p&gt;
    
    </summary>
    
    
      <category term="MySQL" scheme="https://zjusct.github.io/tags/MySQL/"/>
    
      <category term="CentOS" scheme="https://zjusct.github.io/tags/CentOS/"/>
    
  </entry>
  
  <entry>
    <title>BP算法</title>
    <link href="https://zjusct.github.io/2018/12/24/bp/"/>
    <id>https://zjusct.github.io/2018/12/24/bp/</id>
    <published>2018-12-24T13:03:59.000Z</published>
    <updated>2019-05-26T08:02:50.194Z</updated>
    
    <content type="html"><![CDATA[<p>本文探讨BP算法。</p><a id="more"></a><h1 id="Feed-forward-neural-network-and-back-propagation"><a href="#Feed-forward-neural-network-and-back-propagation" class="headerlink" title="Feed forward neural network and back propagation"></a>Feed forward neural network and back propagation</h1><h2 id="1-Neuron-structure"><a href="#1-Neuron-structure" class="headerlink" title="1. Neuron structure"></a>1. Neuron structure</h2><p><img src="neuron.png" alt="Neuron structure"></p><p>上图是一种典型的神经元结构，$x_n$是神经元的输入，将输入加权求和后再通过激活函数即可得到此神经元的输出：<br>$$t = \sum_{i=1}^{n}{w_ix_i} + b$$<br>$$a = f(t)$$</p><p>为计算方便，可将偏置$b$提到求和符号里面，相当于加入一个恒为1的输入值，对应的权重为$b$：<br>$$t = \sum_{i=0}^{n}{w_ix_i},(x_0 = 1, w_0 = b)$$<br>$$a = f(t)$$<br>此即为上图神经元结构对应的表达式</p><p>常用的激活函数有sigmoid, ReLU, tanh等。</p><h2 id="2-Network-structure"><a href="#2-Network-structure" class="headerlink" title="2. Network structure"></a>2. Network structure</h2><p><img src="bp_net.jpg" alt="Network Structure"></p><p>这是一个简单的3层网络，输入层有3个输入值，隐藏层包含3个隐藏神经元，最后是两个输出值<br>隐藏层神经元的前向计算过程：</p><p>$$z_i^{l} = \sum_{i=0}^{n}w_{ij}^{l}x_j, (x_0 = 1, w_0 = b)$$</p><p>$$a_i^l = f(z_i^l)$$</p><p>$l$表示第几层。</p><p>这个网络的抽象数学表达式为：<br>$$F(x) = f_3(f_2(x * W_2 + b_2) * W_3 + b_3)$$</p><p>事实上，深度神经网络一般都能够抽象为一个复合的非线性多元函数，有多少隐藏层就有多少层复合函数：<br>$$F(x) = f_n\left(\dots f_3(f_2(f_1(x) * w_1 + b_1) * w_2 + b_2)\dots\right)$$</p><h2 id="3-Loss"><a href="#3-Loss" class="headerlink" title="3. Loss"></a>3. Loss</h2><p>Loss，即损失，用来衡量神经网络的输出值与实际值的误差，对于不同的问题，通常会定义不同的loss函数</p><p>回归问题常用的均方误差：<br>$$MSE = \frac{1}{n}\sum_{i=1}^{n}(Y - f(x))^2$$<br>$Y$为实际值，$f(x)$为网络预测值</p><p>分类问题常用的交叉熵(m类)：<br>$$L = \sum_{k=1}^{n}\sum_{i=1}^{m}l_{ki}log(p_{ki})$$<br>$l_{ki}$表示第k个样本实际是否属于第i类（0，1编码），$p_{ki}$表示第k个样本属于第i类的概率值</p><p>特别地，二分类问题的交叉熵损失函数形式为：<br>$$L = \sum_{i=1}^{n}[y_ilog(p_i) + (1 - y_i)log(1 - p_i)]$$<br>$y_i$为第i个样本所属类别，$p_i$为第i个样本属于$y_i$类的概率</p><h2 id="4-Back-propagation"><a href="#4-Back-propagation" class="headerlink" title="4. Back propagation"></a>4. Back propagation</h2><p>BP 是用来将loss反向传播的算法，用来调整网络中神经元间连接的权重和偏置，整个训练的过程就是：前向计算网络输出<br>-&gt;;根据当前网络输出计算loss-&gt;BP算法反向传播loss调整网络参数，不断循环这样的三步直到loss达到最小或达到指定停止条件</p><p>BP算法的本质是求导的链式法则，对于上面的三层网络，假设其损失函数为$C$，激活函数为$\sigma$，第$l$第$i$个神经元的输入为$z_i^{(l)}$，输出为$a_i^{(l)}$</p><p>则通过梯度下降来更新权值和偏置的公式如下：<br>$$W_{ij}^{(l)} = W_{ij}^{(l)} - \eta\frac{\partial}{\partial W_{ij}^{(l)}}C\tag1$$<br>$$b_{i}^{(l)} = b_{i}^{(l)} - \eta\frac{\partial}{\partial b_{i}^{(l)}}C\tag2$$</p><p>$W_{ij}^{(l)}$表示第$l$层第$i$个神经元与第$l - 1$层第$j$个神经元连接的权值，$b_i^{(l)}$表示第$l$层第$i$个神经元的偏置</p><p>$\eta$表示学习率</p><p>由更新公式可见主要问题在于求解损失函数关于权值和偏置的偏导数</p><p>第$l$层第$i$个神经元的输入$z_i^{(l)}$为：<br>$$z_i^{(l)} = \sum_{j=1}^{n^{(l-1)}}{W_{ij}^{(l)}a_j^{(l-1)}} + b_i^{l}\tag3$$</p><p>则更新公式中偏导项可化为:</p><p>$$\frac{\partial}{\partial W_{ij}^{(l)}}C = \frac{\partial C}{\partial z_i^{(l)}} \bullet \frac{\partial z_i^{(l)}}{\partial W_{ij}^{(l)}} = \frac{\partial C}{\partial z_i^{(l)}} \bullet a_i^{(l-1)}\tag4$$</p><p>$$\frac{\partial}{\partial b_{i}^{(l)}}C = \frac{\partial C}{\partial z_i^{(l)}} \bullet \frac{\partial z_i^{(l)}}{\partial b_{i}^{(l)}} = \frac{\partial C}{\partial z_i^{(l)}}\tag5$$</p><p>定义</p><p>$$\delta_i^{(l)} = \frac{\partial}{\partial z_i^{(l)}}C\tag6$$</p><p>现在问题转化为求解$\delta_i^{(l)}$，对第$l$层第$j$个神经元有：<br>$$<br>\delta_j^{(l)} = \frac{\partial C}{\partial z_j^{(l)}} = \sum_{i=1}^{n^{(l+1)}}\frac{\partial C}{\partial z_i^{(l+1)}} \bullet \frac{\partial z_i^{(l+1)}}{\partial a_j^{(l)}} \bullet \frac{\partial a_j^{(l)}}{\partial z_j^{(l)}} \<br>=\sum_{i=1}^{n^{(l+1)}}\delta_i^{(l+1)} \bullet \frac{\partial(W_{ij}^{l+1} + b_i^{(l+1)})}{\partial a_j^{(l)}} \bullet \sigma^\prime(z_j^{(l)})\<br>=\sum_{i=1}^{n^{(l+1)}}\delta_i^{(l+1)} \bullet W_{ij}^{(l+1)} \bullet \sigma^\prime(z_j^{(l)})\tag7<br>$$</p><p>则：<br>$$\delta^{(l)} = ((W^{(l+1)})^T\delta^{(l+1)})\odot\sigma^\prime(z^{(l)})\tag8$$</p><p>损失函数关于权重和偏置的偏导分别为：<br>$$\frac{\partial C}{\partial W_{ij}^{(l)}} = a_i^{(l-1)}\delta_i^{(l)}\tag9$$<br>$$\frac{\partial C}{\partial b_{i}^{(l)}} =\delta_i^{(l)}\tag{10}$$</p><p>误差根据8式由输出层向后传播，再结合1，2，9，10四式对权重和偏置进行更新</p><h2 id="5-实现"><a href="#5-实现" class="headerlink" title="5.实现"></a>5.实现</h2><p>下面是一个简单3隐层神经网络的实现</p><p>In [ ]:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.sum((pred - y) ** <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_prime</span><span class="params">(pred, y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> pred - y</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">network</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, num_layers, output_size, loss = loss, loss_prime = loss_prime)</span>:</span></span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.output_size = output_size</span><br><span class="line"></span><br><span class="line">        <span class="comment"># activation function</span></span><br><span class="line">        self.activation = self.sigmoid</span><br><span class="line">        <span class="comment"># derivative of activation function</span></span><br><span class="line">        self.activation_prime = self.sigmoid_prime</span><br><span class="line">        <span class="comment"># loss funciton</span></span><br><span class="line">        self.loss = loss</span><br><span class="line">        <span class="comment"># derivative of loss function</span></span><br><span class="line">        self.loss_prime = loss_prime</span><br><span class="line"></span><br><span class="line">        <span class="comment"># input-&gt;hidden</span></span><br><span class="line">        self.w_ih = np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.b_ih = np.random.randn(<span class="number">1</span>, hidden_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hidden layers</span></span><br><span class="line">        self.W_hh = [np.random.randn(hidden_size, hidden_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers - <span class="number">1</span>)]</span><br><span class="line">        self.B_hh = [np.random.randn(<span class="number">1</span>, hidden_size) <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_layers - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># hidden-&gt;output</span></span><br><span class="line">        self.w_ho = np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.b_ho = np.random.randn(<span class="number">1</span>, output_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># assemble w and b</span></span><br><span class="line">        self.W = [self.w_ih]</span><br><span class="line">        self.W.extend(self.W_hh)</span><br><span class="line">        self.W.append(self.w_ho)</span><br><span class="line"></span><br><span class="line">        self.B = [self.b_ih]</span><br><span class="line">        self.B.extend(self.B_hh)</span><br><span class="line">        self.B.append(self.b_ho)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># activation</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(x) * (<span class="number">1</span> - self.sigmoid(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward pass, calculate the output of the network</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, a)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> w, b <span class="keyword">in</span> zip(self.W, self.B):</span><br><span class="line">            a = self.activation(np.dot(a, w) + b)</span><br><span class="line">        <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backpropagate error</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        delta_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.W]</span><br><span class="line">        delta_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.B]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get output of each layer in forward pass</span></span><br><span class="line">        out = x</span><br><span class="line">        outs = []</span><br><span class="line">        zs = []</span><br><span class="line">        <span class="keyword">for</span> w, b <span class="keyword">in</span> zip(self.W, self.B):</span><br><span class="line">            z = np.dot(out, w) + b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            out = self.activation(z)</span><br><span class="line">            outs.append(out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># δ of last layer</span></span><br><span class="line">        delta = self.loss_prime(outs[<span class="number">-1</span>], y) * self.activation_prime(zs[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">        delta_b[<span class="number">-1</span>] = delta</span><br><span class="line">        delta_w[<span class="number">-1</span>] = np.dot(outs[<span class="number">-2</span>].transpose(), delta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, len(delta_w)):</span><br><span class="line">            delta = np.dot(delta, self.W[-i+<span class="number">1</span>].transpose()) * self.activation_prime(zs[-i])</span><br><span class="line">            delta_b[-i] = delta</span><br><span class="line">            delta_w[-i] = np.dot(outs[-i<span class="number">-1</span>].transpose(), delta)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> delta_w, delta_b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># update w and b</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, batch, lr)</span>:</span></span><br><span class="line">        delta_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.W]</span><br><span class="line">        delta_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.B]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> batch:</span><br><span class="line">            d_w, d_b = self.backward(x, y)</span><br><span class="line">            delta_w = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> zip(delta_w, d_w)]</span><br><span class="line">            delta_b = [a + b <span class="keyword">for</span> a, b <span class="keyword">in</span> zip(delta_b, d_b)]</span><br><span class="line"></span><br><span class="line">        self.W = [w - lr * t <span class="keyword">for</span> w, t <span class="keyword">in</span> zip(self.W, delta_w)]</span><br><span class="line">        self.B = [b - lr * t <span class="keyword">for</span> b, t <span class="keyword">in</span> zip(self.B, delta_b)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SGD training</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, train_data, epochs, batch_size, lr)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(epochs):</span><br><span class="line">            np.random.shuffle(train_data)</span><br><span class="line">            batches = [train_data[t : t + batch_size] <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">0</span>, len(train_data), batch_size)]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> batch <span class="keyword">in</span> batches:</span><br><span class="line">                self.update(batch, lr)</span><br><span class="line"></span><br><span class="line">            loss = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x, y <span class="keyword">in</span> train_data:</span><br><span class="line">                loss += self.loss(self.forward(x), y)</span><br><span class="line">            loss /= len(train_data)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">"Epoch %d done, loss: %f"</span> % (i + <span class="number">1</span>, loss))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># predict</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.forward(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># use it for handwriting digits classification</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">onehot</span><span class="params">(y)</span>:</span></span><br><span class="line">    arr = np.zeros([y.shape[<span class="number">0</span>], <span class="number">10</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(y.shape[<span class="number">0</span>]):</span><br><span class="line">        arr[i][y[i]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"></span><br><span class="line">(x_train, y_train),(x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span></span><br><span class="line">x_train = x_train.reshape([<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>])</span><br><span class="line">x_test = x_test.reshape([<span class="number">-1</span>, <span class="number">28</span> * <span class="number">28</span>])</span><br><span class="line">y_train = onehot(y_train)</span><br><span class="line">y_test = onehot(y_test)</span><br><span class="line"></span><br><span class="line">train_data = [t <span class="keyword">for</span> t <span class="keyword">in</span> zip(x_train, y_train)]</span><br><span class="line">test_data = [t <span class="keyword">for</span> t <span class="keyword">in</span> zip(x_test, y_test)]</span><br><span class="line"></span><br><span class="line">input_size = <span class="number">28</span> * <span class="number">28</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line">num_layers = <span class="number">3</span></span><br><span class="line">output_size = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">net = network(input_size, hidden_size, num_layers, output_size)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.005</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">net.train(train_data, epochs, batch_size, lr)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax</span><span class="params">(x)</span>:</span></span><br><span class="line">    exp = np.exp(x)</span><br><span class="line">    <span class="keyword">return</span> exp / np.sum(exp)</span><br><span class="line"></span><br><span class="line">correct = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> test_data:</span><br><span class="line">    ret = net.forward(x)</span><br><span class="line">    pred = softmax(ret)</span><br><span class="line">    <span class="keyword">if</span> np.argmax(pred) == np.argmax(y):</span><br><span class="line">        correct += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">acc = float(correct) / len(test_data)</span><br><span class="line">print(<span class="string">'test accuracy: '</span>, acc)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文探讨BP算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Machine Learning" scheme="https://zjusct.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Distributed Tensorflow</title>
    <link href="https://zjusct.github.io/2018/12/23/tensorflow/"/>
    <id>https://zjusct.github.io/2018/12/23/tensorflow/</id>
    <published>2018-12-23T11:57:25.000Z</published>
    <updated>2019-05-26T08:02:50.201Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-单机"><a href="#1-单机" class="headerlink" title="1.单机"></a>1.单机</h2><h3 id="log-device-placement"><a href="#log-device-placement" class="headerlink" title="log_device_placement"></a>log_device_placement</h3><p>单机情况比较简单，不需要特殊配置，TensorFlow会自动将计算任务分配到可用的GPU上，在定义session时，可以通过<em>log_device_placement</em>参数来打印具体的计算任务分配：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'b'</span>)</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(config = tf.ConfigProto(log_device_placement = <span class="literal">True</span>)) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(c))</span><br></pre></td></tr></table></figure><img src="1.png"><h3 id="指定设备"><a href="#指定设备" class="headerlink" title="指定设备"></a>指定设备</h3><p>如果需要让一些运算在特定的设备上执行，可以使用tf.device:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">a = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'a'</span>)</span><br><span class="line">b = tf.constant([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>], shape=[<span class="number">3</span>], name=<span class="string">'b'</span>)</span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">c = a + b</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(config = tf.ConfigProto(log_device_placement = <span class="literal">True</span>)) <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    print(sess.run(c))</span><br></pre></td></tr></table></figure><img src="2.png"><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>尽管上面一个例子中我们只给CPU和GPU0指定了计算任务，但是两块显卡的显存都被占满了：</p><img src="3.png"><p>因为TensorFlow会默认占满所有可见GPU的显存，对于简单的计算任务，这样显然非常浪费，我们可以通过修改环境变量<em>CUDA_VISIBLE_DEVICES</em>解决这个问题:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 运行时指定环境变量</span></span><br><span class="line">CUDA_VISIBLE_DEVICES=0 python demo.py</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python 代码中修改环境变量</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>]=<span class="string">'0'</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h2 id="2-多机"><a href="#2-多机" class="headerlink" title="2.多机"></a>2.多机</h2><h3 id="In-graph-amp-Between-graph"><a href="#In-graph-amp-Between-graph" class="headerlink" title="In-graph &amp; Between-graph"></a>In-graph &amp; Between-graph</h3><p>TensorFlow的分布式训练有两种模式：In-graph和Between-graph</p><p>In-graph: 不同的机器执行计算图的不同部分，和单机多GPU模式类似，一个节点负责模型数据分发，其他节点等待接受任务，通过<em>tf.device(“/job:worker/task:n”)</em>来指定计算运行的节点</p><img src="5.png"><p>Between-graph:每台机器执行相同的计算图</p><blockquote><p>Author: 陈岩<br>PostDate: 2018.12.21</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-单机&quot;&gt;&lt;a href=&quot;#1-单机&quot; class=&quot;headerlink&quot; title=&quot;1.单机&quot;&gt;&lt;/a&gt;1.单机&lt;/h2&gt;&lt;h3 id=&quot;log-device-placement&quot;&gt;&lt;a href=&quot;#log-device-placement&quot; cla
      
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Tensorflow" scheme="https://zjusct.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Quick Guide to CUDA Profiling</title>
    <link href="https://zjusct.github.io/2018/12/07/cuprof/"/>
    <id>https://zjusct.github.io/2018/12/07/cuprof/</id>
    <published>2018-12-07T15:48:33.000Z</published>
    <updated>2019-05-26T08:02:50.197Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Brief-Introduction"><a href="#1-Brief-Introduction" class="headerlink" title="1. Brief Introduction"></a>1. Brief Introduction</h2><p>在并行计算领域，很难通过纯理论的分析来确定程序的性能，<code>GPGPU</code>这种基于特定计算架构的计算任务更甚。事实上，很多制约并行算法性能的瓶颈很可能不在算法本身（比如资源调度障碍）。因此，对给定程序进行充分的性能测试与后续分析是相当必要的调优方法。</p><p><code>Nvidia</code>提供了<code>nvprof</code>，<code>nvvp</code>，<code>Nsight</code>三种cuda可用的性能分析工具，本文将简述配合使用<code>nvprof</code>与<code>nvvp</code>的cuda程序性能分析方法。</p><a id="more"></a><h2 id="2-Check-Out-Device-Properties"><a href="#2-Check-Out-Device-Properties" class="headerlink" title="2. Check Out Device Properties"></a>2. Check Out Device Properties</h2><p>由于cuda程序的线程/块分配方案与程序运行的的硬件高度相关，故对目标平台的硬件参数有一定程度的了解是相当有必要的。我们可以使用<code>cudaGetDeviceProperties()</code>函数获取设备的各项属性，下述代码可以结合<code>cuda_runtime_api.h#1218</code>处<code>struct cudaDeviceProp</code>的定义和各属性的相应注解自行理解。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> nDevices;</span><br><span class="line">cudaDeviceProp prop;</span><br><span class="line">cudaGetDeviceCount( &amp;nDevices );</span><br><span class="line"><span class="keyword">for</span> ( <span class="keyword">auto</span> i = <span class="number">0</span>; i != nDevices; ++i )</span><br><span class="line">&#123;</span><br><span class="line">cudaGetDeviceProperties( &amp;prop, i );</span><br><span class="line"><span class="comment">// check out interesting property</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-Profile-Using-Nvprof"><a href="#3-Profile-Using-Nvprof" class="headerlink" title="3. Profile Using Nvprof"></a>3. Profile Using Nvprof</h2><h3 id="3-1-Quick-Start"><a href="#3-1-Quick-Start" class="headerlink" title="3.1. Quick Start"></a>3.1. Quick Start</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvprof --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><h3 id="3-2-Metrics"><a href="#3-2-Metrics" class="headerlink" title="3.2. Metrics"></a>3.2. Metrics</h3><ul><li>使用<code>--query-metrics</code>列出所有可测试的性能指标。</li><li>使用<code>--metrics sm_efficiency,warp_execution_efficiency,...</code>指定要测试的性能指标。</li></ul><h3 id="3-3-PC-Sampling"><a href="#3-3-PC-Sampling" class="headerlink" title="3.3. PC Sampling"></a>3.3. PC Sampling</h3><p>在CC5.2或更高的设备上支持使用PC采样(PC sampling)技术。</p><p>PC采样技术通过<code>Round-Robin</code>方法对SM中所有活动线程束的PC状态进行采样，采样结果包含如下两种可能：</p><ul><li>线程束完成了当前指令。</li><li>线程束被<code>stall</code>，不能完成当前指令，并可以给出<code>stall</code>的原因。</li></ul><p>事实上线程束被<code>stall</code>并不代表指令流水线处于<code>stall</code>状态，因为其他正常运行的线程束可以利用计算资源。</p><p>CC6.0以上的设备对PC采样方法进行了改进，通过检查线程束调度器是否执行指令来确定指令流水线是否真正处于<code>stall</code>状态，从而能正确指示指令<code>stall</code>的原因。</p><h2 id="4-Data-Visualize-Using-Nvvp"><a href="#4-Data-Visualize-Using-Nvvp" class="headerlink" title="4. Data Visualize Using Nvvp"></a>4. Data Visualize Using Nvvp</h2><p><code>nvvp</code>可以导入<code>nvprof</code>的分析结果，可视化显示统计图表，并且建议性地指出程序可能存在的瓶颈。</p><p><em>以饼状图显示各类stall比重</em></p><img src="a.jpg"><p><em>以频谱显示各类指令比例</em></p><img src="b.jpg"><p><em>通过source file mapping可视化指令stall状态，需要在编译选项中指定<code>-lineinfo</code></em></p><img src="d.jpg"><h3 id="4-1-Usage"><a href="#4-1-Usage" class="headerlink" title="4.1. Usage"></a>4.1. Usage</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvprof -f --kernels <span class="string">"kernelName"</span> --analysis-metrics -o a.nvvp &lt;task&gt; &lt;args&gt;</span><br><span class="line">nvvp a.nvvp</span><br></pre></td></tr></table></figure><p>这里我使用的方法是在集群上用<code>nvprof</code>做性能测试，之后将分析结果<code>*.nvvp</code>传回本地用<code>nvvp</code>做可视化。</p><h2 id="Ext-Remarks"><a href="#Ext-Remarks" class="headerlink" title="Ext. Remarks"></a>Ext. Remarks</h2><h3 id="Tradeoff-Between-Registers-and-Threads"><a href="#Tradeoff-Between-Registers-and-Threads" class="headerlink" title="Tradeoff Between Registers and Threads"></a>Tradeoff Between Registers and Threads</h3><p>在实际Profiling中重新认识了这个问题。</p><p>在默认情况下，<code>nvcc</code>为每个线程分配<code>maxRegsPerThread</code>个数的寄存器，在<em>Tesla K40</em>上，这个值为64。同时，每个SM持有为65536个寄存器，这意味着单个SM中的线程数最多不超过1024。通过检查参数表，我们发现该设备单个SM可容纳线程数为2048。这意味着我们计算任务的GPU利用率最大只有50%（所有SM均满载的状态下）。</p><p>在这种情况下，如果我们将分配给单个线程的寄存器数目减半，则最大GPU利用率可以达到100%。但若发生寄存器溢出（register spilling），溢出的存储空间被放到片外的local memory，访问速度在（同在片外的）global memory级别。</p><p>在实际的CUDA核函数中，能全部利用64个寄存器的情况很少。寄存器的使用情况可以在nvvp中检查，如果发现有大量寄存器浪费，可以立即减少寄存器数量。在大多数情况下，可以结合计算任务的量级和性质来调节线程最大寄存器数，从而达到有针对性的性能调优。</p><p>在<code>nvcc</code>中指定单个线程最大寄存器数，可以添加编译选项<code>-maxrregcount=N</code>。如果限定不修改编译选项或需要逐核函数指定，则需要使用<code>__launch_bounds__</code>限定符，如下（隐式地指定了最大寄存器个数）：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="keyword">void</span></span><br><span class="line">__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)</span><br><span class="line">MyKernel(...)</span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在我的<code>path tracer</code>中对上述方法进行测试，将每线程的寄存器数减半为32，SM线程数加倍并满载，GPU利用率由30+提升到70+，执行速度有1.5倍左右的提升。</p><h3 id="Tradeoff-Between-BlockDim-and-BlockPerSM"><a href="#Tradeoff-Between-BlockDim-and-BlockPerSM" class="headerlink" title="Tradeoff Between BlockDim and BlockPerSM"></a>Tradeoff Between BlockDim and BlockPerSM</h3><p>当一个块（block）中的所有线程束（warp）全部完成时，这个块才可以被SM调度。如果块的大小过大，则块的运行速度受单个线程束约束的开销就越大（如果算法并行度很高，增大块的大小不失为一个好选择）；如果块的大小过小，则一方面SM可能无法达到其最大利用率（受<code>maxBlocksPerSM</code>的限制），另一方面SM调度块的额外开销也会增大。尤其是针对不同特点的计算任务有不同的更优选择，如<code>divergency</code>较高的任务更适合较小的BlockDim。所以在选择BlockDim时不仅要在算法的适应性上做考虑，还要通过多次性能测试来进行针对性的优化。</p><h3 id="Beware-of-Ladder-Effects"><a href="#Beware-of-Ladder-Effects" class="headerlink" title="Beware of Ladder Effects"></a>Beware of Ladder Effects</h3><p>注意计算资源分配时要注意分配的资源量要能够被组别整除，否则会出现断层状的资源浪费现象。</p><p><em>每块线程数与SM中最大线程束数的关系</em></p><img src="c.jpg">]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Brief-Introduction&quot;&gt;&lt;a href=&quot;#1-Brief-Introduction&quot; class=&quot;headerlink&quot; title=&quot;1. Brief Introduction&quot;&gt;&lt;/a&gt;1. Brief Introduction&lt;/h2&gt;&lt;p&gt;在并行计算领域，很难通过纯理论的分析来确定程序的性能，&lt;code&gt;GPGPU&lt;/code&gt;这种基于特定计算架构的计算任务更甚。事实上，很多制约并行算法性能的瓶颈很可能不在算法本身（比如资源调度障碍）。因此，对给定程序进行充分的性能测试与后续分析是相当必要的调优方法。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Nvidia&lt;/code&gt;提供了&lt;code&gt;nvprof&lt;/code&gt;，&lt;code&gt;nvvp&lt;/code&gt;，&lt;code&gt;Nsight&lt;/code&gt;三种cuda可用的性能分析工具，本文将简述配合使用&lt;code&gt;nvprof&lt;/code&gt;与&lt;code&gt;nvvp&lt;/code&gt;的cuda程序性能分析方法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="CUDA" scheme="https://zjusct.github.io/tags/CUDA/"/>
    
      <category term="Profile" scheme="https://zjusct.github.io/tags/Profile/"/>
    
  </entry>
  
  <entry>
    <title>CUDA内存管理总结(一)</title>
    <link href="https://zjusct.github.io/2018/11/25/cuda/"/>
    <id>https://zjusct.github.io/2018/11/25/cuda/</id>
    <published>2018-11-24T16:46:08.000Z</published>
    <updated>2019-05-26T08:02:50.196Z</updated>
    
    <content type="html"><![CDATA[<p>本文将探讨CUDA中的内存管理机制。</p><a id="more"></a><h2 id="一、寄存器"><a href="#一、寄存器" class="headerlink" title="一、寄存器"></a>一、寄存器</h2><p>​    GPU的每个SM（流多处理器）都有上千个寄存器，每个SM都可以看作是一个多线程的CPU核，但与一般的CPU拥有二、四、六或八个核不同，一个GPU可以有<strong>N个SM核</strong>；同样，与一般的CPU核支持一到两个硬件线程不同，每个SM核可能有<strong>8~192个SP</strong>（流处理器），亦即每个SM能同时支持这么多个硬件线程。事实上，一台GPU设备的所有SM中活跃的线程数目通常数以万计。</p><h3 id="1-1-寄存器映射方式"><a href="#1-1-寄存器映射方式" class="headerlink" title="1.1 寄存器映射方式"></a>1.1 寄存器映射方式</h3><p>​    <strong>CPU处理多线程</strong>：进行上下文切换，使用寄存器重命名机制，将当前所有寄存器的状态保存到栈（系统内存），再从栈中恢复当前需要执行的新线程在上一次的执行状态。这些操作通常花费上百个CPU时钟周期，有效工作吞吐量低。</p><p>​    <strong>GPU处理多线程</strong>：与CPU相反，GPU利用多线程隐藏了内存获取与指令执行带来的延迟；此外，GPU不再使用寄存器重命名机制，而是尽可能为每个线程分配寄存器，从而上下文切换就变成了寄存器组选择器（或指针）的更新，几乎是零开销。</p><h3 id="1-2-寄存器空间大小"><a href="#1-2-寄存器空间大小" class="headerlink" title="1.2 寄存器空间大小"></a>1.2 寄存器空间大小</h3><p>​    每个SM可提供的寄存器空间大小分别有8KB、16KB、32KB和64KB，每个线程中的每个变量占用一个寄存器，因而总共会占用N个寄存器，N代表调度的线程数量。当线程块上的寄存器数目是允许的最大值时，每个SM会只处理一个线程块。</p><h3 id="1-3-SM调度线程、线程块"><a href="#1-3-SM调度线程、线程块" class="headerlink" title="1.3 SM调度线程、线程块"></a>1.3 SM调度线程、线程块</h3><p>​    由于大多数内核对寄存器的需求量很低，所以可以通过降低寄存器的需求量来增加SM上线程块的调度数量，从而提高运行的线程总数，根据线程级并行<strong>“占用率越高，程序运行越快”</strong>，可以实现运行效率的优化。当线程级并行（<em>Thread-Level Parallelism</em>，TLP）足以隐藏存储延迟时会达到一个临界点，此后想要继续提高程序性能，可以在单个线程中实现指令级的并行（<em>Instruction-Level Parallelism</em>，ILP），即单线程处理多数据。</p><p>​    但在另一方面，每个SM所能调度的线程总量是有限制的，因此当线程总量达到最大时，再减少寄存器的使用量就无法达到提高占有率的目的（如下表中寄存器数目由20减小为16，线程块调度数量不变），所以在这种情况下，应增加寄存器的使用量到临界值。</p><img src="1.png"><h3 id="1-4-寄存器优化方式"><a href="#1-4-寄存器优化方式" class="headerlink" title="1.4 寄存器优化方式"></a>1.4 寄存器优化方式</h3><p>​    1）将中间结果累积在寄存器而非全局内存中。尽量避免全局内存的写操作，因为如果操作聚集到同一块内存上，就会强制硬件对内存的操作序列化，导致严重的性能降低；</p><p>​    2）循环展开。循环一般非常低效，因为它们会产生分支，造成流水线停滞。</p><h3 id="1-5-总结"><a href="#1-5-总结" class="headerlink" title="1.5 总结"></a>1.5 总结</h3><p>​    使用寄存器可以有效消除内存访问，或提供额外的ILP，以此实现GPU内核函数的加速，这是最为有效的方法之一。</p><h2 id="二、共享内存"><a href="#二、共享内存" class="headerlink" title="二、共享内存"></a>二、共享内存</h2><h3 id="2-1-基本概念"><a href="#2-1-基本概念" class="headerlink" title="2.1 基本概念"></a>2.1 基本概念</h3><p>​    1、共享内存实际上是可以受用户控制的一级缓存，每个SM中的一级缓存和共享内存共用一个64KB的内存段。</p><p>​    2、共享内存的延迟很低，大约有1.5TB/s的带宽，而全局内存仅为160GB/s，换言之，有效利用共享内存有可能获得7倍的加速比。但它的速度依然只有寄存器的十分之一，并且共享内存的速度几乎在所有GPU中都相同，因为它由核时钟频率驱动。</p><p>​    3、只有当数据重复利用、全局内存合并，或者线程之间有共享数据（例如同时访问相同地址的存储体）的时候使用共享内存才更合适，否则将数据直接从全局内存加载到寄存器性能会更好。</p><p>​    4、共享内存是基于存储体切换的架构（<em>bank-switched architecture</em>），费米架构的设备上有32个存储体。无论有多少线程发起操作，<strong>每个</strong>存储体<strong>每个</strong>周期只执行<strong>一次</strong>操作。因此，如果线程束中的每个线程各访问一个存储体，那么所有线程的操作都可以在一个周期内同时执行，且所有操作都是独立互不影响的。此外，如果所有线程同时访问同一地址的存储体，会触发一个广播机制到线程束中的每个线程中。但是，如果是其他的访问方式，线程访问共享内存就需要排队，即一个线程访问时，其他线程将阻塞闲置。因此很重要的一点时，应该尽可能地获得<strong>零存储体冲突</strong>的共享内存访问。</p><h3 id="2-2-Example：使用共享内存排序"><a href="#2-2-Example：使用共享内存排序" class="headerlink" title="2.2 Example：使用共享内存排序"></a>2.2 Example：使用共享内存排序</h3><h2 id="2-2-1-归并排序"><a href="#2-2-1-归并排序" class="headerlink" title="2.2.1 归并排序"></a>2.2.1 归并排序</h2><p>​    假设待排序的数据集大小为N，现将数据集进行划分。根据归并排序的划分原则，最后每个数据包中只有两个数值需要排序，因此，在这一阶段，最大并行度可达到 $N \over 2$ 个独立线程。例如，处理一个大小为512KB的数据集，共有128K个32位的元素，那么最多可以使用的线程个数为64K个（N=128K，N/2=64K），假设GPU上有16个SM，每个SM最多支持1536个线程，那么每个GPU上最多可以支持24K个线程，因此，按照这样划分，64K的数据对只需要2.5次迭代即可完成排序操作。</p><p>​    但是，如果采用上述划分排序方式再进行合并，我们需要从每个排好序的数据集中读出元素，对于一个64K的集合，需要64K次读操作，即从内存中获取256MB的数据，显然当数据集很大的时候不合适。</p><p>​    因此，我们采用通过限制对原始问题的迭代次数，通过基于共享内存的分解方式来获得更好的合并方案。因为在费米架构的设备上有32个存储体，即对应32个线程，所以当需要的线程数量减少为32（一个线程束）时，停止迭代，于是共需要线程束4K个（128K/32=4K），又因为GPU上有16个SM，所以这将为每个SM分配到256个线程束。然而由于费米架构设备上的每个SM最多只能同时执行48个线程束，因此多个块将被循环访问。</p><p>​    通过将数据集以每行32个元素的方式在共享内存中进行分布，每列为一个存储体，即可得到零存储体冲突的内存访问，然后对每一列实施相同的排序算法。（或者也可以理解为桶排序呀）</p><p>​    然后再进行列表的合并。</p><h2 id="2-2-2-合并列表"><a href="#2-2-2-合并列表" class="headerlink" title="2.2.2 合并列表"></a>2.2.2 合并列表</h2><p>​    先从串行合并任意数目的有序列表看起：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge_array</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, <span class="comment">//待排序数组</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 u32 *<span class="keyword">const</span> dest_array, <span class="comment">//排序后的数组</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 <span class="keyword">const</span> u32 num_lists, <span class="comment">//列表总数</span></span></span></span><br><span class="line"><span class="function"><span class="params">                 <span class="keyword">const</span> u32 num_elements)</span> <span class="comment">//数据总数</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);<span class="comment">//每个列表中的数据个数</span></span><br><span class="line">    u32 list_indexes[MAX_NUM_LISTS]; <span class="comment">//所有列表当前所在的元素下标</span></span><br><span class="line">    <span class="keyword">for</span>(u32 <span class="built_in">list</span> = <span class="number">0</span>; <span class="built_in">list</span> &lt; num_lists; <span class="built_in">list</span>++)</span><br><span class="line">    &#123;</span><br><span class="line">list_indexes[<span class="built_in">list</span>] = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">dest_array[i] = find_min(scr_array, </span><br><span class="line">                                 list_indexes, </span><br><span class="line">                                 num_lists, </span><br><span class="line">                                 num_elements_per_list);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">u32 <span class="title">find_min</span><span class="params">(<span class="keyword">const</span> u32*cosnt src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">             u32 *<span class="keyword">const</span> list_indexes, </span></span></span><br><span class="line"><span class="function"><span class="params">             <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">             <span class="keyword">const</span> u32 num_elements_per_list)</span><span class="comment">//寻找num_lists个元素中的最小值</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    u32 min_val = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">    u32 min_idx = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i &lt; num_lists; i++)</span><br><span class="line">    &#123;</span><br><span class="line"><span class="keyword">if</span>(list_indexes[i] &lt; num_elements_per_list)</span><br><span class="line">        &#123;</span><br><span class="line"><span class="keyword">const</span> u32 src_idx = i + (list_indexes[i]*num_lists);</span><br><span class="line">             <span class="keyword">const</span> u32 data = src_array[src_idx];</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(data &lt;= min_val)</span><br><span class="line">        &#123;</span><br><span class="line">min_val = data;</span><br><span class="line">                min_idx = i;</span><br><span class="line">        &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    list_indexes[min_idx]++;</span><br><span class="line">    <span class="keyword">return</span> min_val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    将上述算法用GPU实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">gpu_sort_array_array</span><span class="params">(u32 *<span class="keyword">const</span> data, </span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                                     <span class="keyword">const</span> u32 num_elements)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> u32 tid = (blockIdx.x * blockDim.x) + threadIdx.x;</span><br><span class="line">    __shared__ u32 sort_tmp[NUM_ELEM];</span><br><span class="line">    __shared__ u32 sort_tmp_1[NUM_ELEM];</span><br><span class="line">    </span><br><span class="line">    copy_data_to_shared(data, sort_tmp, num_lists, num_elements, tid);</span><br><span class="line">    radix_sort2(sort_tmp, num_lists, num_elements, tid, sort_tmp_1);</span><br><span class="line">    merge_array6(sort_tmp, data, num_lists, num_elements, tid);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    第一个函数的实现：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">copy_data_to_shared</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> data, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    u32 *sort_tmp, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                                    <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i &lt; num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">sort_tmp[i+tid] = data[i+tid]; </span><br><span class="line">    &#125;</span><br><span class="line">    __syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    该函数中，程序按行将数据从全局内存读入共享内存。当函数调用一个子函数并传入参数时，这些参数必须以某种方式提供给被调用的函数，有两种方法可以采用。一种是通过寄存器传递所需的值，另一种方法是创建一个名为“栈帧”的内存区，但这种方法非常地不高效。出于这一原因，我们需要重新修改合并的程序(merge_array)，以避免函数调用，修改后程序如下（单线程）：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array1</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">__shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    </span><br><span class="line">    lists_indexes[tid] = <span class="number">0</span>;<span class="comment">//从每个列表的第一个元素开始</span></span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//单线程</span></span><br><span class="line">    <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);</span><br><span class="line">        <span class="keyword">for</span>(u32 i = <span class="number">0</span>; i &lt; num_elements; i++)</span><br><span class="line">        &#123;</span><br><span class="line">u32 min_val = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">             u32 min_idx = <span class="number">0</span>;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span>(u32 <span class="built_in">list</span> = <span class="number">0</span>; <span class="built_in">list</span> &lt; num_lists; <span class="built_in">list</span>++)</span><br><span class="line">      &#123;</span><br><span class="line"><span class="keyword">if</span>(list_indexes[<span class="built_in">list</span>] &lt; num_elements_per_list)</span><br><span class="line">        &#123;</span><br><span class="line">   <span class="keyword">const</span> u32 src_idx = i + (list_indexes[i]*num_lists);</span><br><span class="line">             <span class="keyword">const</span> u32 data = src_array[src_idx];</span><br><span class="line">        <span class="keyword">if</span>(data &lt;= min_val)</span><br><span class="line">        &#123;</span><br><span class="line">min_val = data;</span><br><span class="line">                 min_idx = i;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          list_indexes[min_idx]++;</span><br><span class="line">          dest_array[i]=min_val;</span><br><span class="line">       &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    这里只用一个线程进行合并，但显然，为了获得更好的性能，一个线程是远远不够的。因为数据被写到一个单一的列表中，所以多个线程必须进行某种形式的合作。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array6</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//每个列表分到的元素个数</span></span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建一个共享列表数组，用来储存当前线程所访问的列表元素下标</span></span><br><span class="line">    __shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    list_indexes[tid] = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//创建所有线程共享的最小值与最小值线程号</span></span><br><span class="line">    __shared__ u32 min_val;</span><br><span class="line">    __shared__ u32 min_tid;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(u32 i=<span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;   </span><br><span class="line">        u32 data;</span><br><span class="line">        <span class="comment">//如果当前列表还未被读完，则从中读取数据</span></span><br><span class="line">        <span class="keyword">if</span>(list_indexes[tid] &lt; num_elements_per_list);</span><br><span class="line">        &#123;</span><br><span class="line">             <span class="comment">//计算出当前元素在原数组中的下标</span></span><br><span class="line"><span class="keyword">const</span> u32 src_idx = tid + (list_indexes[tid] * num_lists);</span><br><span class="line">             data = src_array[src_idx];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">data = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//用零号线程来初始化最小值与最小值线程号</span></span><br><span class="line">        <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">min_val = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">             min_tid = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//让所有线程都尝试将它们现在手上有的值写入min_val，但只有最小的数据会被保留</span></span><br><span class="line">        <span class="comment">//利用__syncthreads()确保每个线程都执行了该操作</span></span><br><span class="line">        atomicMin(&amp;min_val, data);</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//在所有data==min_val的线程中，选取最小线程号写入min_tid</span></span><br><span class="line">        <span class="keyword">if</span>(min_val == data)</span><br><span class="line">        &#123;</span><br><span class="line">atomicMin(&amp;min_tid, tid);</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将满足要求的线程所在列表的当前元素往后移一位，进行下一轮比较</span></span><br><span class="line">        <span class="comment">//并将筛选结果存入结果数组dest_array</span></span><br><span class="line">        <span class="keyword">if</span>(tid == min_tid)</span><br><span class="line">        &#123;</span><br><span class="line">list_indexes[tid]++;</span><br><span class="line">             dest_array[i] = data;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    上面的函数中将num_lists个线程进行合并操作，但只用了一个线程一次将结果写入结果数据数组中，保证了结果的正确性，不会引起线程间的冲突。</p><p>​    其中使用到了 atomicMin 函数。每个线程以从列表中获取的数据作为入参调用该函数，取代了原先单线程访问列表中所有元素并找出最小值的操作。当每个线程调用 atomicMin 函数时，线程读取保存在共享内存中的最小值并于当前线程中的值进行比较，然后把比较结果重新写回最小值对应的共享内存中，同时更新最小值对应的线程号。然而，由于列表中的数据可能会重复，因此可能出现多个线程的值均为最小值的情况，保留的线程号却各不相同。因此需要执行第二步操作，保证保留的线程号为最小线程号。</p><p>​    虽然这种方法的优化效果很显著，但它也有一定的劣势。例如，atomicMin函数只能用在计算能力为1.2以上的设备上；另外，aotomicMin函数只支持整数型运算，但现实世界中的问题通常是基于浮点运算的，因此在这种情况下，我们需要寻找新的解决方法。</p><h2 id="2-2-3-并行归约"><a href="#2-2-3-并行归约" class="headerlink" title="2.2.3 并行归约"></a>2.2.3 并行归约</h2><p>​    并行归约适用于许多问题，求最小值只是其中的一种。它使用数据集元素数量一半的线程，每个线程将当前线程对应的元素与另一个元素进行比较，计算两者之间的最小值，并将得到的最小值移到前面。每进行一次比较，线程数减少一半，如此反复直到只剩一个元素为止，这个元素就是需要的最小值。</p><p>​    在选择比较元素的时候，应该尽量避免选择同一个线程束中的元素进行比较，因为这会明显地导致线程束内产生分支，而每个分支都将使SM做双倍的工作，继而影响程序的性能。因此我们选择将线程束中的元素与另一半数据集中的元素进行比较。如下图，阴影部分表示当前活跃的线程。</p><img src="2.png"><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array5</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_lists,</span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">const</span> u32 num_elements_per_list = (num_elements / num_lists);</span><br><span class="line">    </span><br><span class="line">    __shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    __shared__ u32 reduction_val[MAX_NUM_LISTS];</span><br><span class="line">    __shared__ u32 reduction_idx[MAX_NUM_LISTS];</span><br><span class="line">    </span><br><span class="line">    list_indexes[tid] = <span class="number">0</span>;</span><br><span class="line">    reduction_val[tid] = <span class="number">0</span>;</span><br><span class="line">    reduction_idx[tid] = <span class="number">0</span>;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(u32 i=<span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">u32 tid_max = num_lists &gt;&gt; <span class="number">1</span>;<span class="comment">//最大线程数为列表总数的一半</span></span><br><span class="line">         u32 data;<span class="comment">//使用寄存器可以提高运行效率，将对共享内存的写操作次数减少为1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">//当列表中还有未处理完的元素时</span></span><br><span class="line">         <span class="keyword">if</span>(list_indexes[tid] &lt; num_elements_per_list)</span><br><span class="line">         &#123;</span><br><span class="line">             <span class="comment">//计算该元素在原数组中的位置</span></span><br><span class="line">cosnst u32 src_idx = tid + (list_indexes[tid] * num_lists);</span><br><span class="line">             data = src_array[src_idx];</span><br><span class="line">         &#125;</span><br><span class="line">        <span class="comment">//若当前列表已经处理完，将data赋值最大</span></span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">data = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将当前元素及线程号写入共享内存</span></span><br><span class="line">        reduction_val[tid] = data;</span><br><span class="line">        reduction_idx[tid] = tid;</span><br><span class="line">        __syncthreads;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//当前活跃的线程数多于一个时</span></span><br><span class="line">        <span class="keyword">while</span>(tid_max!=<span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(tid &lt; tid_max)</span><br><span class="line">            &#123;</span><br><span class="line">                 <span class="comment">//将当前线程中的元素与另一半数据集中的对应元素进行比较</span></span><br><span class="line"><span class="keyword">const</span> u32 val2_idx = tid + tid_max;</span><br><span class="line">                 <span class="keyword">const</span> u32 val2 = reduction_val[val2_idx];</span><br><span class="line">                 </span><br><span class="line">                 <span class="comment">//最后保留较小的那个元素</span></span><br><span class="line">                 <span class="keyword">if</span>(reduction_val[tid] &gt; val2)</span><br><span class="line">                 &#123;</span><br><span class="line">reduction_val[tid] = val2;</span><br><span class="line">                      reduction_idx[tid] = reduction_idx[val_idx];</span><br><span class="line">                 &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//线程数减半，进入下一轮循环</span></span><br><span class="line">            tid_max &gt;&gt;= <span class="number">1</span>;</span><br><span class="line">            __syncthreads();</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//在零号线程中将结果写入结果数组，并将相应线程所指的元素后移一位</span></span><br><span class="line">        <span class="keyword">if</span>(tid == <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">list_indexes[reduction_idx[<span class="number">0</span>]]++;</span><br><span class="line">             dest_array[i] = reduction_val[<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    同样，这种方法也在共享内存中创建了一个临时的列表 list_indexes 用来保存每次循环中从 num_list 个数据集列表中选取出来进行比较的数据。如果进行合并的列表已经为空，那么就将临时列表中的对应数据区赋最大值0xFFFFFFFF。而每轮while循环后，活跃的线程数都将减少一半，直到最后只剩一个活跃的线程，亦即零号线程。最后将结果复制到结果数组中并将最小值所对应的列表索引加一，以确保元素不会被处理两次。</p><h2 id="2-2-4-混合算法"><a href="#2-2-4-混合算法" class="headerlink" title="2.2.4 混合算法"></a>2.2.4 混合算法</h2><p>​    在了解atomicMin函数和并行归约两种方案后，我们可以利用这两种算法各自的优点，创造出一种新的混合方案。</p><p>​    简单的1~N个数据归约的一个主要问题就是当N增大时，程序的速度先变快再变慢，达到最高效的情形时N在8至16左右。混合算法将原数据集划分成诸多个小的数据集，分别寻找每块中的最小值，然后再将每块得到的结果最终归约到一个值中。这种方法和并行归约的思想非常相似，但同时又省略了并行归约中的多次迭代。代码更新如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REDUCTION_SIZE 8</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> REDUCTION_SIZE_BIT_SHIFT 3</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAX_ACTIVE_REDUCTIONS ((MAX_NUM_LISTS) / (REDUCTION_SIZE))</span></span><br><span class="line"></span><br><span class="line">__<span class="function">device__ <span class="keyword">void</span> <span class="title">merge_array</span><span class="params">(<span class="keyword">const</span> u32 *<span class="keyword">const</span> src_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                            u32 *<span class="keyword">const</span> dest_array, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> u32 num_lists, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> u32 num_elements, </span></span></span><br><span class="line"><span class="function"><span class="params">                            <span class="keyword">const</span> u32 tid)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//每个线程都从原数组中读入一个数据，用作首次比较</span></span><br><span class="line">    u32 data = src_array[tid];</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//当前线程所在的数据块编号（8个线程为一组，每个线程处理一个列表）</span></span><br><span class="line">    <span class="keyword">const</span> u32 s_idx = tid &gt;&gt; REDUCTION_SIZE_BIT_SHIFT;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//首次进行分别归约的数据块总数</span></span><br><span class="line">    <span class="keyword">const</span> u32 num_reductions = num_lists &gt;&gt; REDUCTION_SIZE_BIT_SHIFT;</span><br><span class="line">    <span class="keyword">const</span> u32 num_elements_per_list = num_elements / num_lists;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//在共享内存中创建一个列表，指向每个线程当前所在的元素，并初始化为0</span></span><br><span class="line">    __shared__ u32 list_indexes[MAX_NUM_LISTS];</span><br><span class="line">    list_indexes[tid] = <span class="number">0</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//遍历所有数据</span></span><br><span class="line">    <span class="keyword">for</span>(u32 i=<span class="number">0</span>; i&lt;num_elements; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//每个数据块在内部归约后都会产生一个相应的最小值</span></span><br><span class="line">        <span class="comment">//在共享内存中开辟一个列表，用来保存每组的最小值</span></span><br><span class="line">__shared__ u32 min_val[MAX_ACTIVE_REDUCTIONS];</span><br><span class="line">         __shared__ u32 min_tid;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//初始化每个数据块的内部最小值</span></span><br><span class="line">        <span class="keyword">if</span>(tid &lt; num_lists)</span><br><span class="line">        &#123;</span><br><span class="line">min_val[s_idx] = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">             min_tid = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        __syncthreads();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//将当前线程的数据与所处数据块的最小值进行比较，并保留较小的那一个</span></span><br><span class="line">        atomicMin(&amp;min_val[s_idx], data);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">//进行归约的数据块总数不为零时</span></span><br><span class="line">        <span class="keyword">if</span>(num_reductions &gt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">//确保每个线程都已经将上一步比较操作完成</span></span><br><span class="line">__syncthreads();</span><br><span class="line">            </span><br><span class="line">             <span class="comment">//将每个数据块产生的最小值与零号数据块的最小值进行比较，保留较小的那一个</span></span><br><span class="line">             <span class="keyword">if</span>(tid &lt; num_reductions)</span><br><span class="line">             &#123;</span><br><span class="line">atomicMin(&amp;min_val[<span class="number">0</span>], min_val[tid]);</span><br><span class="line">                  __syncthreads();</span><br><span class="line">             &#125;</span><br><span class="line">            </span><br><span class="line">             <span class="comment">//如果当前线程的数据等于此次比较保留的最小值，记录最小线程号</span></span><br><span class="line">             <span class="keyword">if</span>(data == min_val[<span class="number">0</span>])</span><br><span class="line">             &#123;</span><br><span class="line">atomicMin(&amp;min_tid, tid);</span><br><span class="line">             &#125;</span><br><span class="line">             <span class="comment">//确保上一步操作每个线程都已经完成，才能执行下一句</span></span><br><span class="line">             __syncthreads();</span><br><span class="line">            </span><br><span class="line">            <span class="comment">//如果当前线程号恰为记录下的最小线程号</span></span><br><span class="line">            <span class="keyword">if</span>(tid == min_tid)</span><br><span class="line">            &#123;</span><br><span class="line">                 <span class="comment">//当前所指元素后移一位</span></span><br><span class="line">list_indexes[tid]++;</span><br><span class="line">                </span><br><span class="line">                 <span class="comment">//将结果保存入结果数组</span></span><br><span class="line">                  dest_array[i] = data;</span><br><span class="line">                 </span><br><span class="line">                  <span class="comment">//若该线程对应的列表尚未被处理完</span></span><br><span class="line">                  <span class="keyword">if</span>(list_indexes[tid] &lt; num_elements_per_list)</span><br><span class="line">                      <span class="comment">//更新该线程的data，进行下一轮比较</span></span><br><span class="line">                      data = src_array[tid + (list_indexes[tid] * num_lists)];</span><br><span class="line">                  <span class="keyword">else</span></span><br><span class="line">                      data = <span class="number">0xFFFFFFFF</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            __syncthreads();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    注意到：</p><p>​    1）原来的min_val由单一的数据扩展成为一个共享数据的数组，这是因为每个独立的线程都需要从它对应的数据集中获取当前的最小值来进行内部比较。每个最小值都是一个32位的数值，因此可以存储在独立的共享内存存储体中。</p><p>​    2）内核函数中的REDUCTION_SIZE的值被设置成8，意味着每个数据块中包含8个数据，程序分别找出每个数据块的最小值，然后再在这些最小值中寻找最终的最小值。</p><p>​    3）内核函数中最重要的一个变化是，只有每次比较的最小值所对应的那个线程的data才会更新，其他线程的data都不会更新。而在之前的内核函数中，每轮比较开始，所有线程都会从对应的列表中重新读入data 的值，随着N的增大，这将变得越来越低效。</p><h2 id="2-2-5-总结"><a href="#2-2-5-总结" class="headerlink" title="2.2.5 总结"></a>2.2.5 总结</h2><p>​    1）共享内存允许同一个线程块中的线程读写同一段内存，但线程看不到也无法修改其他线程块的共享内存。</p><p>​    2）共享内存的缓冲区驻留在物理GPU上，所以访问时的延迟远低于访问普通缓冲区的延迟，因此除了使用寄存器，还应更有效地使用共享内存，尤其当数据有重复利用，或全局内存合并，或线程间有共享数据的时候。</p><p>​    3）编写代码时，将关键字_shared__添加到声明中，使得该变量留驻在共享内存中，并且线程块中的每个线程都可以共享这块内存，使得一个线程块中的多个线程能够在计算上进行通信和协作。</p><p>​    4）调用 __syncthreads() 函数来实现线程的同步操作，尤其要注意确保在读取共享内存之前，想要写入的操作都已经完成。另外还需要注意，切不可将这个函数放置在发散分支（某些线程需要执行，而其他线程不需要执行），因为除非线程块中的每个线程都执行了该函数，没有任何线程能够执行之后的指令，从而导致死锁。</p><p>​    5）不妨尝试使用共享内存实现矩阵乘法的优化。</p><blockquote><p>Author: 潘薇鸿<br>PostDate: 2018.11.25</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将探讨CUDA中的内存管理机制。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="CUDA" scheme="https://zjusct.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>浙江大学超算队博客...活了？</title>
    <link href="https://zjusct.github.io/2018/09/30/first/"/>
    <id>https://zjusct.github.io/2018/09/30/first/</id>
    <published>2018-09-30T13:25:55.000Z</published>
    <updated>2019-05-26T08:02:50.200Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>这里是浙江大学超算队的官方博客</p><a id="more"></a><p>记录一些前沿的Tech姿势、大家平时的DeBug经历、还有ASC世界超算大赛的经验分享！</p><p>希望这个博客不仅仅是给以后的小盆友们提供宝贵的经验 更是诸君技术的记录与提升的大好机会</p><p>平时我们写着代码 总是不愿意写文档 遇到了非常Tricky又神奇的Bug 花了一下午终于解决了 却没有记录下来</p><p>很有可能很多的无谓的时间就会在以后被重复 这样的时间浪费 我们完全可以节省下来！</p><p>希望大家能享受在超算队一起学习的时光~ 从现在开始 把这段美好的时间变成字符 记录下来叭~~</p><img src="ttfish.jpeg"><blockquote><p>Author: TTfish<br>PostDate: 2018.9.30</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;这里是浙江大学超算队的官方博客&lt;/p&gt;
    
    </summary>
    
    
      <category term="Tech" scheme="https://zjusct.github.io/tags/Tech/"/>
    
      <category term="Spc" scheme="https://zjusct.github.io/tags/Spc/"/>
    
      <category term="ZJU" scheme="https://zjusct.github.io/tags/ZJU/"/>
    
  </entry>
  
</feed>
